{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOP31Gz4BzCN",
        "outputId": "6beb125f-1ce8-41d8-efb3-ff21da4e9cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zFv2hZJB5YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d82680-1043-4633-d244-616fd7b03d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NN - Spring 2023/HW2\n",
            "'Copy of main (3).py'   HW2_exps.ipynb\t\t     oneLayer\t     resnet18\n",
            " dataset\t        images\t\t\t     PetImages\t     vgg16\n",
            " densenet121\t        kagglecatsanddogs_5340.zip   randomForrest\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/NN - Spring 2023/HW2\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TEl8DQuB7PLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import tqdm\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from torchvision.models.feature_extraction import create_feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "EDDExrVXPpCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dG3EOq_d9s84"
      },
      "outputs": [],
      "source": [
        "def mkdir(directory: str):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "def plot_stats(train_loss_hist, test_loss_hist, \n",
        "               train_acc_hist, test_acc_hist,\n",
        "               title: str,\n",
        "               path: str=None, name_to_save: str=None, save_only=False):\n",
        "    plt.clf()\n",
        "    # plt.rcParams[\"figure.figsize\"] = (16,7)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_loss_hist, label = 'train loss')\n",
        "    plt.plot(test_loss_hist, label = 'test loss')\n",
        "    plt.title('loss')\n",
        "    plt.legend()\n",
        "    # plt.plot()\n",
        "    # plt.show()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_acc_hist, label = 'train acc')\n",
        "    plt.plot(test_acc_hist, label = 'test acc')\n",
        "    plt.title('acc')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.suptitle(f'{title}')\n",
        "    if not path is None:\n",
        "      mkdir(path)\n",
        "      plt.savefig(f'{path}/{name_to_save}.png')\n",
        "    \n",
        "    if not save_only:\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L5yD-m2BeeW"
      },
      "outputs": [],
      "source": [
        "# !wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBhusZ2_Bh6y"
      },
      "outputs": [],
      "source": [
        "# !unzip -q kagglecatsanddogs_5340.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "mYCZa3zaPtrn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j0_FjEvSncng"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/54049440/speed-up-datasets-loading-on-google-colab\n",
        "def get_data(batch_size: int = 8, randseed: int = 11, pin_memory=False,\n",
        "             pret_model_data: str = 'ImageNet', num_workers=1):\n",
        "    def seed_worker(worker_id):\n",
        "        # worker_seed = torch.initial_seed() % 2 ** 32\n",
        "        np.random.seed(randseed)\n",
        "        random.seed(randseed)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(randseed)\n",
        "\n",
        "    def is_valid(filename: str):\n",
        "        corrupted = [\n",
        "            'PetImages/Cat/Thumbs.db',\n",
        "            'PetImages/Dog/Thumbs.db',\n",
        "            'PetImages/Cat/666.jpg',\n",
        "            'PetImages/Dog/11702.jpg'\n",
        "        ]\n",
        "        return not (filename in corrupted)\n",
        "    \n",
        "    # from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "    if pret_model_data == 'ImageNet':\n",
        "        img_dimensions = 224\n",
        "        # https://discuss.pytorch.org/t/understanding-transform-normalize/21730\n",
        "        # https://stackoverflow.com/a/75986472\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((img_dimensions, img_dimensions)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "        ])\n",
        "    \n",
        "    data_dir = 'PetImages'\n",
        "    ds = datasets.ImageFolder(data_dir, transform, is_valid_file = is_valid)\n",
        "    \n",
        "    # https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split\n",
        "    train_ds, test_ds = torch.utils.data.random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
        "    print(len(train_ds), len(test_ds))\n",
        "\n",
        "    full_data_loaders = {\n",
        "        'train': DataLoader(train_ds, batch_size=batch_size, \n",
        "                            worker_init_fn=seed_worker,\n",
        "                            generator=g,\n",
        "                            shuffle=True,\n",
        "                            pin_memory=pin_memory,\n",
        "                            num_workers=num_workers),\n",
        "        'val': DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
        "                          pin_memory=pin_memory,\n",
        "                          num_workers=num_workers)\n",
        "    }\n",
        "    class_names = ds.classes\n",
        "\n",
        "    return full_data_loaders, class_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments1 Base Class"
      ],
      "metadata": {
        "id": "PREEEWQeP0Bu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PvHHo2pj_0JW"
      },
      "outputs": [],
      "source": [
        "class exp():\n",
        "    def __init__(self, model_name: str, freeze_mode: int,\n",
        "                full_data_loaders,\n",
        "                 num_epochs: int, warmup_steps: int,  \n",
        "                 batch_size: int = 32, \n",
        "                 opt: str = 'adam', lr: float = 0.001,  weight_decay=1e-5,\n",
        "                 warmup_lr: float = 0.001, \n",
        "                 num_classes: int = 2, randseed: int = 11) -> None:\n",
        "        \n",
        "        self.seed = randseed\n",
        "        torch.manual_seed(self.seed)\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        \n",
        "        self.full_data_loaders = full_data_loaders\n",
        "\n",
        "        self.freeze_mode = freeze_mode\n",
        "        self.model_name = model_name\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.opt_str = opt\n",
        "        self.lr = lr\n",
        "        self.warmup_lr = warmup_lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.num_classes = num_classes\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "        self.model: nn.Module\n",
        "        self.input_size: int\n",
        "        self.params_to_update_str: List[str]\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.name = f'model_{model_name}-freeze_{freeze_mode}-optim_{opt}-{lr}-num_epochs_{num_epochs}-warmup_{self.warmup_steps}'\n",
        "        self.init_env()\n",
        "\n",
        "    def train_model(self, save_best = True): \n",
        "        # from https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#enable-cudnn-auto-tuner\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        \n",
        "        phases = ['train', 'val']\n",
        "        # phases = ['train']\n",
        "        \n",
        "        since = time.time()\n",
        "\n",
        "        val_acc_hist = []\n",
        "        train_acc_hist = []\n",
        "        val_loss_hist = []\n",
        "        train_loss_hist = []\n",
        "\n",
        "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        best_val_acc = 0.0\n",
        "        best_val_acc_loss = 0.0\n",
        "        best_epoch = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print('Epoch {}/{}'.format(epoch, self.num_epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "            if epoch == self.warmup_steps: \n",
        "                custom_set_parameter_requires_grad(self.model, self.model_name, self.freeze_mode)\n",
        "                params_to_update = self.get_params_to_update()\n",
        "                self.set_opt(params_to_update, False)\n",
        "\n",
        "            for phase in phases:\n",
        "                if phase == 'train':\n",
        "                    self.model.train() \n",
        "                else:\n",
        "                    self.model.eval()   \n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                total_start = time.time()\n",
        "                model_time = 0.0\n",
        "                for inputs, labels in tqdm(self.full_data_loaders[phase]):\n",
        "                    model_start = time.time()\n",
        "\n",
        "                    inputs = inputs.to(self.device)\n",
        "                    labels = labels.to(self.device)\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = self.model(inputs)\n",
        "                        loss = self.criterion(outputs, labels)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            self.optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                    model_time += time.time() - model_start\n",
        "                    # print(model_time)\n",
        "\n",
        "                print('total and model time and dataTime ', time.time() - total_start, model_time, (time.time() - total_start) -  model_time)\n",
        "\n",
        "                epoch_loss = running_loss / len(self.full_data_loaders[phase].dataset)\n",
        "                epoch_acc = running_corrects.double() / len(self.full_data_loaders[phase].dataset)\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_val_acc:\n",
        "                    best_val_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "                    best_epoch = epoch\n",
        "                    best_val_acc_loss = epoch_loss\n",
        "                if phase == 'val':\n",
        "                    val_acc_hist.append(epoch_acc)\n",
        "                    val_loss_hist.append(epoch_loss)\n",
        "                elif phase == 'train':\n",
        "                    train_acc_hist.append(epoch_acc)\n",
        "                    train_loss_hist.append(epoch_loss)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best val Acc: {:4f}'.format(best_val_acc), f'at epoch {best_epoch}')\n",
        "\n",
        "        # load best model weights\n",
        "        self.model.load_state_dict(best_model_wts)\n",
        "\n",
        "        train_acc_hist = torch.asarray(train_acc_hist).cpu().detach().numpy()\n",
        "        train_loss_hist = torch.asarray(train_loss_hist).cpu().detach().numpy()\n",
        "        val_acc_hist = torch.asarray(val_acc_hist).cpu().detach().numpy()\n",
        "        val_loss_hist = torch.asarray(val_loss_hist).cpu().detach().numpy()\n",
        "        \n",
        "        # SAVE\n",
        "        mkdir(self.model_name)\n",
        "\n",
        "        if save_best: \n",
        "            torch.save({\n",
        "                'epoch': best_epoch,\n",
        "                'model_name': self.model_name,\n",
        "                'model': self.model,\n",
        "                'optimizer': self.optimizer,\n",
        "                'val_loss': best_val_acc_loss,\n",
        "                'val_acc': best_val_acc, \n",
        "            }, f'{self.model_name}/{self.name}-model.pt')\n",
        "\n",
        "        with open(f'{self.model_name}/{self.name}-stats.pickle', 'wb') as handle:\n",
        "            pickle.dump({\n",
        "                'model_name': self.model_name,\n",
        "                'freeze_mode': self.freeze_mode,\n",
        "                'num_epochs': self.num_epochs,\n",
        "                'lr': self.lr,\n",
        "                'optim': self.opt_str,\n",
        "                'val_loss_hist': val_loss_hist,\n",
        "                'val_acc_hist': val_acc_hist,\n",
        "                'train_loss_hist': train_loss_hist, \n",
        "                'train_acc_hist': train_acc_hist,                 \n",
        "            }, handle)\n",
        "\n",
        "        plot_stats(train_loss_hist, val_loss_hist,\n",
        "                   train_acc_hist, val_acc_hist,\n",
        "                   f'{self.name}', 'images', f'{self.name}')\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def get_params_to_update(self):\n",
        "        # get learnable parameters: \n",
        "        params_to_update = self.model.parameters()\n",
        "        params_to_update_str = []\n",
        "        print(\"Params to learn:\")\n",
        "\n",
        "        params_to_update = []\n",
        "        for name,param in self.model.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\",name)\n",
        "                params_to_update_str.append(name)\n",
        "\n",
        "        self.params_to_update = params_to_update\n",
        "        self.params_to_update_str = params_to_update_str\n",
        "\n",
        "        return params_to_update\n",
        "\n",
        "    def set_opt(self, params_to_update, is_warmup: bool):\n",
        "        lr_opt: float\n",
        "        if is_warmup:\n",
        "            lr_opt = self.warmup_lr\n",
        "        else:\n",
        "            lr_opt = self.lr\n",
        "\n",
        "        if self.opt_str == 'sgd':\n",
        "            self.optimizer = optim.SGD(params_to_update, lr=lr_opt, momentum=0.9, weight_decay=self.weight_decay)\n",
        "        elif self.opt_str == 'adam':\n",
        "            self.optimizer = optim.Adam(params_to_update, lr=lr_opt, weight_decay=self.weight_decay)\n",
        "        else: \n",
        "            print('invalid optim')\n",
        "            exit()\n",
        "\n",
        "    def init_env(self):\n",
        "        # model config\n",
        "        self.model, self.input_size = self.init_model()\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "        params_to_update = self.get_params_to_update()\n",
        "        # optims\n",
        "        self.set_opt(params_to_update, True)\n",
        "\n",
        "        # loss func\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # data loading \n",
        "        # self.full_data_loaders, class_names = get_data(self.batch_size, self.seed)\n",
        "\n",
        "    def init_model(self, use_pretrained=True):\n",
        "        # from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "        model_ft = None\n",
        "        input_size = 0\n",
        "        num_classes = self.num_classes\n",
        "        \n",
        "        # NO NON-LINEARITY in the last classifer, \n",
        "        # add two layers\n",
        "        #  nn.Sequential(nn.Linear(model_resnet18.fc.in_features,512),\n",
        "                                #   nn.ReLU(),\n",
        "                                #   nn.Dropout(),\n",
        "                                #   nn.Linear(512, num_classes))\n",
        "\n",
        "        if self.model_name == 'oneLayer': \n",
        "            model_ft = oneLayer(224*224*3, 2)\n",
        "            # model_ft = oneLayer(784, 10)\n",
        "            \n",
        "            input_size = 224\n",
        "\n",
        "        if self.model_name == \"resnet18\":\n",
        "            \"\"\" Resnet18\n",
        "            \"\"\"\n",
        "            model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "            custom_set_parameter_requires_grad(model_ft, self.model_name, 100)\n",
        "            num_ftrs = model_ft.fc.in_features\n",
        "            model_ft.fc = nn.Sequential(\n",
        "                nn.Linear(num_ftrs, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.8),\n",
        "                nn.Linear(256, num_classes),\n",
        "            )\n",
        "            input_size = 224\n",
        "\n",
        "        elif self.model_name == \"vgg16\":\n",
        "            \"\"\" VGG16_bn\n",
        "            \"\"\"\n",
        "            model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
        "            custom_set_parameter_requires_grad(model_ft, self.model_name, 100)\n",
        "            num_ftrs = model_ft.classifier[0].in_features\n",
        "            model_ft.classifier = nn.Sequential(\n",
        "                nn.Linear(num_ftrs, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.8),\n",
        "                nn.Linear(256, num_classes),\n",
        "            )\n",
        "            input_size = 224\n",
        "    \n",
        "        elif self.model_name == \"densenet121\":\n",
        "            \"\"\" Densenet121\n",
        "            \"\"\"\n",
        "            model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "            custom_set_parameter_requires_grad(model_ft, self.model_name, 100)\n",
        "            num_ftrs = model_ft.classifier.in_features\n",
        "            model_ft.classifier = nn.Sequential(\n",
        "                nn.Linear(num_ftrs, 256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=0.8),\n",
        "                nn.Linear(256, num_classes),\n",
        "            )\n",
        "            input_size = 224\n",
        "        else:\n",
        "            print(\"Invalid model name, exiting...\")\n",
        "            # exit()\n",
        "        \n",
        "        print(model_ft)\n",
        "        return model_ft, input_size\n",
        "\n",
        "def custom_set_parameter_requires_grad(model: nn.Module, model_name: str, freeze_mode):\n",
        "    # first freeze all layers: \n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    if model_name == 'densenet121':\n",
        "        unfreeze_layers = []\n",
        "        if freeze_mode <= 100:\n",
        "            for name, param in model.named_parameters():\n",
        "                if 'classifier' in name: \n",
        "                    param.requires_grad = True\n",
        "                    \n",
        "        if freeze_mode <= 70:\n",
        "            unfreeze_layers.append('norm5') # useless\n",
        "            unfreeze_layers.append('denseblock4')\n",
        "        if freeze_mode <= 50: \n",
        "            # ? \n",
        "            pass\n",
        "        if freeze_mode <= 30: \n",
        "            unfreeze_layers.append('transition3')\n",
        "            unfreeze_layers.append('denseblock3')\n",
        "        if freeze_mode <= 10: \n",
        "            unfreeze_layers.append('transition2')\n",
        "            unfreeze_layers.append('denseblock2')\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            unfreeze = False\n",
        "            for layerName in unfreeze_layers: \n",
        "                if layerName in name:\n",
        "                    unfreeze = True    \n",
        "            if unfreeze: \n",
        "                param.requires_grad = True\n",
        "            \n",
        "    elif model_name == 'vgg16':\n",
        "        unfreeze_layers_indices = []\n",
        "        if freeze_mode <= 100:\n",
        "            for name, param in model.named_parameters():\n",
        "                if 'classifier' in name: \n",
        "                    param.requires_grad = True\n",
        "\n",
        "        if freeze_mode <= 70:\n",
        "            #  L5\n",
        "            #  34-43\n",
        "            unfreeze_layers_indices.extend(np.arange(34, 43+1).tolist())\n",
        "        if freeze_mode <= 50: \n",
        "            # L4\n",
        "            # 24-33\n",
        "            unfreeze_layers_indices.extend(np.arange(24, 33+1).tolist())\n",
        "        if freeze_mode <= 30: \n",
        "            # L3\n",
        "            # 14-23\n",
        "            unfreeze_layers_indices.extend(np.arange(14, 23+1).tolist())\n",
        "        if freeze_mode <= 10: \n",
        "            # L2\n",
        "            # 7-13\n",
        "            unfreeze_layers_indices.extend(np.arange(7, 13+1).tolist())\n",
        "\n",
        "        for layer_idx in unfreeze_layers_indices:\n",
        "            for param in model.features[layer_idx].parameters():\n",
        "                param.requires_grad = True    \n",
        "\n",
        "    elif model_name == 'resnet18':\n",
        "        unfreeze_layers = []\n",
        "        if freeze_mode <= 100:\n",
        "            unfreeze_layers.append('fc') \n",
        "        if freeze_mode <= 70:\n",
        "            unfreeze_layers.append('avgpool') # useless\n",
        "            unfreeze_layers.append('layer4')\n",
        "        if freeze_mode <= 50: \n",
        "            unfreeze_layers.append('layer3')\n",
        "        if freeze_mode <= 30: \n",
        "            unfreeze_layers.append('layer2')\n",
        "        if freeze_mode <= 10: \n",
        "            unfreeze_layers.append('layer1')\n",
        "        \n",
        "        for name, param in model.named_parameters():\n",
        "            unfreeze = False\n",
        "            for layerName in unfreeze_layers: \n",
        "                if layerName in name:\n",
        "                    unfreeze = True    \n",
        "            if unfreeze: \n",
        "                param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAb2OmA52F0W"
      },
      "source": [
        "# Experiments1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjRRjJXg4sZP"
      },
      "source": [
        "## ResNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### resnet with warmups: "
      ],
      "metadata": {
        "id": "VmRrlhLL3jzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lr_dict = {\n",
        "    100: 0.001,\n",
        "    70: 0.0005,\n",
        "    50: 0.0001,\n",
        "    30: 0.00005,\n",
        "    10: 0.00001,\n",
        "}\n",
        "opt = 'sgd'\n",
        "batch_size = 64\n",
        "randseed = 11\n",
        "warmup_steps = 1\n",
        "\n",
        "full_loaders, _ = get_data(batch_size, randseed, pin_memory=True, num_workers=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6y5T1F3n54",
        "outputId": "34b6d502-159b-4946-bfbd-bf914e3a816b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "for model_name in ['resnet18']:\n",
        "    for freeze_mode in [10, 30, 50, 70, 100]:\n",
        "        if freeze_mode == 50 and model_name == 'densenet121': \n",
        "            continue\n",
        "        print('-'*20)\n",
        "        print('freeze_mode: ', freeze_mode)\n",
        "        curr_exp = exp(model_name=model_name, freeze_mode=freeze_mode,\n",
        "                        full_data_loaders = full_loaders, warmup_steps = warmup_steps,\n",
        "                         num_epochs=num_epochs, batch_size=batch_size, \n",
        "                         opt=opt, lr = lr_dict[freeze_mode], warmup_lr = 0.001, randseed=randseed)\n",
        "        best_models.append(curr_exp.train_model())"
      ],
      "metadata": {
        "id": "SnuftYOX3pxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V_QzI2gkd18"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vgg with warmup"
      ],
      "metadata": {
        "id": "3pPPGS8GqL8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lr_dict = {\n",
        "    100: 0.001,\n",
        "    70: 0.0005,\n",
        "    50: 0.0001,\n",
        "    30: 0.00005,\n",
        "    10: 0.00001,\n",
        "}\n",
        "opt = 'sgd'\n",
        "batch_size = 64\n",
        "randseed = 11\n",
        "warmup_steps = 1\n",
        "\n",
        "full_loaders, _ = get_data(batch_size, randseed, pin_memory=True, num_workers=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIu8jMGa1MPG",
        "outputId": "cf95f84f-ac32-48ac-8202-7d96bd131021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = []\n",
        "for model_name in ['vgg16']:\n",
        "    for freeze_mode in [100, 70, 50, 30, 10]: \n",
        "        if freeze_mode == 50 and model_name == 'densenet121': \n",
        "            continue\n",
        "        print('-'*20)\n",
        "        print('freeze_mode: ', freeze_mode)\n",
        "        curr_exp = exp(model_name=model_name, freeze_mode=freeze_mode,\n",
        "                        full_data_loaders = full_loaders, warmup_steps = warmup_steps,\n",
        "                         num_epochs=num_epochs, batch_size=batch_size, \n",
        "                         opt=opt, lr = lr_dict[freeze_mode], warmup_lr = 0.001, randseed=randseed)\n",
        "        best_models.append(curr_exp.train_model())"
      ],
      "metadata": {
        "id": "crGeb7vpafKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyAthuj2wtyU"
      },
      "source": [
        "## DenseNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet w/warmup"
      ],
      "metadata": {
        "id": "APAWurm6yVQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lr_dict = {\n",
        "    100: 0.001,\n",
        "    70: 0.0005,\n",
        "    50: 0.0001,\n",
        "    30: 0.00005,\n",
        "    10: 0.00001,\n",
        "}\n",
        "opt = 'sgd'\n",
        "batch_size = 64\n",
        "randseed = 11\n",
        "warmup_steps = 1\n",
        "\n",
        "full_loaders, _ = get_data(batch_size, randseed, pin_memory=True, num_workers=5)"
      ],
      "metadata": {
        "id": "Ma05wknXqHvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3Nnc9Rh8LR5"
      },
      "outputs": [],
      "source": [
        "best_models = []\n",
        "for model_name in ['densenet121']:\n",
        "    for freeze_mode in [100, 70, 50, 30, 10]: \n",
        "        if freeze_mode == 50 and model_name == 'densenet121': \n",
        "            continue\n",
        "        print('-'*20)\n",
        "        print('freeze_mode: ', freeze_mode)\n",
        "        curr_exp = exp(model_name=model_name, freeze_mode=freeze_mode,\n",
        "                        full_data_loaders = full_loaders, warmup_steps = warmup_steps,\n",
        "                         num_epochs=num_epochs, batch_size=batch_size, \n",
        "                         opt=opt, lr = lr_dict[freeze_mode], warmup_lr = 0.001, randseed=randseed)\n",
        "        best_models.append(curr_exp.train_model())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyJMqsLUpNPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments2 utils"
      ],
      "metadata": {
        "id": "skId6VIqQWzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feats_dense(model, full_loaders, model_name):\n",
        "    assert model_name == 'densenet121'\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    features = {'train': [], 'val': []}\n",
        "    all_labels = {'train': [], 'val': []}\n",
        "    \n",
        "    # hook\n",
        "    def get_activation():\n",
        "        def hook(model, input, output):\n",
        "            features[phase].append(input[0].cpu().detach().numpy())\n",
        "        return hook\n",
        "\n",
        "    # best_model.layer4[1].bn2\n",
        "    model.classifier[0].register_forward_hook(get_activation())\n",
        "\n",
        "    debugged = False\n",
        "    model = model.to(device)\n",
        "    for phase in ['val', 'train']:\n",
        "        model.eval()\n",
        "\n",
        "        for inputs, labels in tqdm(full_loaders[phase]):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outs = model(inputs)\n",
        "\n",
        "                if not debugged:\n",
        "                    print(features[phase][0].shape)\n",
        "                    debugged = True\n",
        "\n",
        "            all_labels[phase].append(labels.cpu().detach().numpy())    \n",
        "\n",
        "    features['train'] = np.concatenate(features['train'])\n",
        "    features['val'] = np.concatenate(features['val'])\n",
        "    all_labels['train'] = np.concatenate(all_labels['train'])\n",
        "    all_labels['val'] = np.concatenate(all_labels['val'])\n",
        "    \n",
        "    return features, all_labels\n",
        "\n",
        "\n",
        "def get_fe_model(model, model_name):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if model_name == 'resnet18':\n",
        "        return_nodes = {\n",
        "            # \"layer4.1.bn2\": \"features\" ,\n",
        "            \"avgpool\": \"features\",  \n",
        "        }\n",
        "        # m = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "    elif model_name == 'vgg16':\n",
        "        return_nodes = {\n",
        "            \"avgpool\": \"features\",  \n",
        "        }\n",
        "    # elif model_name == 'densenet121':\n",
        "    #     return_nodes = {\n",
        "    #         \"features\": \"features\",  \n",
        "    #     }\n",
        "    \n",
        "    fe_model = create_feature_extractor(model, return_nodes=return_nodes)\n",
        "    return fe_model\n",
        "\n",
        "def get_feats(fe_model, full_loaders, model_name):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    features = {'train': [], 'val': []}\n",
        "    all_labels = {'train': [], 'val': []}\n",
        "    \n",
        "    debugged = False\n",
        "\n",
        "    fe_model = fe_model.to(device)\n",
        "    for phase in ['val', 'train']:\n",
        "        fe_model.eval()\n",
        "\n",
        "        for inputs, labels in tqdm(full_loaders[phase]):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                feats = fe_model(inputs)['features']\n",
        "\n",
        "                if not debugged:\n",
        "                    print(feats.shape)\n",
        "                    debugged = True\n",
        "\n",
        "                features[phase].append(feats.cpu().detach().numpy())\n",
        "                all_labels[phase].append(labels.cpu().detach().numpy())    \n",
        "\n",
        "\n",
        "    features['train'] = np.concatenate(features['train'])\n",
        "    features['val'] = np.concatenate(features['val'])\n",
        "    all_labels['train'] = np.concatenate(all_labels['train'])\n",
        "    all_labels['val'] = np.concatenate(all_labels['val'])\n",
        "    \n",
        "\n",
        "    if model_name == 'resnet18':\n",
        "        features['train'] = features['train'].reshape((features['train'].shape[0], features['train'].shape[1]))\n",
        "        features['val'] = features['val'].reshape((features['val'].shape[0], features['val'].shape[1]))\n",
        "        \n",
        "    elif model_name == 'vgg16':\n",
        "        print(features['train'].shape)\n",
        "        features['train'] = features['train'].reshape((features['train'].shape[0], features['train'].shape[1]*features['train'].shape[2]*features['train'].shape[3]))\n",
        "        features['val'] = features['val'].reshape((features['val'].shape[0], features['val'].shape[1]*features['val'].shape[2]*features['val'].shape[3]))\n",
        "        \n",
        "    return features, all_labels\n",
        "\n",
        "def test_model(model, fdl):\n",
        "    model.eval()   \n",
        "    running_corrects = 0\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for inputs, labels in tqdm(fdl['val']):\n",
        "        model_start = time.time()\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_acc = running_corrects.double() / len(fdl['val'].dataset)\n",
        "    print(epoch_acc)\n",
        "    print(running_corrects.double(), len(fdl['val'].dataset))\n",
        "    return epoch_acc"
      ],
      "metadata": {
        "id": "ksdd8_q9qov0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments2 Base class: "
      ],
      "metadata": {
        "id": "kTS8IrVMQgzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForrestCLF:\n",
        "    def __init__(self, model_name: str, freeze_mode: int, full_loaders):\n",
        "        self.model_name = model_name\n",
        "        self.freeze_mode = freeze_mode\n",
        "        self.best_saved: Dict\n",
        "        self.base_dir = 'randomForrest'\n",
        "        self.full_loaders = full_loaders\n",
        "        mkdir(self.base_dir)\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    def load_model(self):\n",
        "        num_epochs = 10\n",
        "        lr_dict = {\n",
        "            100: 0.001,\n",
        "            70: 0.0005,\n",
        "            50: 0.0001,\n",
        "            30: 5e-05,\n",
        "            10: 1e-05,\n",
        "        }\n",
        "        opt = 'sgd'\n",
        "        batch_size = 64\n",
        "        randseed = 11\n",
        "        warmup_steps = 1\n",
        "\n",
        "        self.best_saved = torch.load(f'{self.model_name}/model_{self.model_name}-freeze_{self.freeze_mode}-optim_{opt}-{lr_dict[self.freeze_mode]}-num_epochs_{num_epochs}-warmup_{warmup_steps}-model.pt')\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'epoch': best_epoch,\n",
        "            'model_name': self.model_name,\n",
        "            'model': self.model,\n",
        "            'optimizer': self.optimizer,\n",
        "            'val_loss': best_val_acc_loss,\n",
        "            'val_acc': best_val_acc, \n",
        "        }, f'{self.model_name}/{self.name}-model.pt')\n",
        "        \"\"\"\n",
        "\n",
        "    def save_feats(self):\n",
        "        \n",
        "        if self.model_name == 'resnet18':\n",
        "            self.feature_extractor = get_fe_model(self.best_saved['model'], self.model_name)\n",
        "            self.features, self.labels = get_feats(self.feature_extractor, self.full_loaders, self.model_name)\n",
        "        elif self.model_name == 'vgg16': \n",
        "            self.feature_extractor = get_fe_model(self.best_saved['model'], self.model_name)\n",
        "            self.features, self.labels = get_feats(self.feature_extractor, self.full_loaders, self.model_name)\n",
        "        elif self.model_name == 'densenet121': \n",
        "            self.features, self.labels = get_feats_dense(self.best_saved['model'], self.full_loaders, self.model_name)\n",
        "\n",
        "        mkdir(f'{self.base_dir}/{self.model_name}')\n",
        "        np.savez(f'{self.base_dir}/{self.model_name}/features_{self.model_name}-{self.freeze_mode}.npz', \n",
        "                 features_train=self.features['train'],\n",
        "                 features_val  =self.features['val'],\n",
        "                 labels_train  =self.labels['train'],\n",
        "                 labels_val    =self.labels['val'])\n",
        "    \n",
        "    def train_clf(self, n_estimators, max_depth):\n",
        "        if not os.path.isfile(f'{self.base_dir}/{self.model_name}/features_{self.model_name}-{self.freeze_mode}.npz'):\n",
        "            print('features from model', self.model_name, 'freeze ', self.freeze_mode, ' not saved, extracting features')\n",
        "            self.load_model()\n",
        "            self.save_feats()\n",
        "\n",
        "        \n",
        "        results = np.load(f'{self.base_dir}/{self.model_name}/features_{self.model_name}-{self.freeze_mode}.npz')\n",
        "        self.features = {\n",
        "            'train': results['features_train'],\n",
        "            'val':   results['features_val']\n",
        "        }\n",
        "        self.labels = {\n",
        "            'train': results['labels_train'],\n",
        "            'val':   results['labels_val']\n",
        "        }\n",
        "\n",
        "        self.clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=11)\n",
        "        self.clf.fit(self.features['train'], self.labels['train'])\n",
        "        acc = self.clf.score(self.features['val'], self.labels['val'])\n",
        "\n",
        "        print(f'using model {self.model_name} with freeze_mode {self.freeze_mode} as feature extractor...')\n",
        "        print(f'RF params: n_estimators {n_estimators}, max_depth {max_depth}')\n",
        "        print('acc: ', acc)"
      ],
      "metadata": {
        "id": "PgOVsT7bQdO_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train RandomForrest"
      ],
      "metadata": {
        "id": "lz5RJOcsV5qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_loaders, _ = get_data(64, 11, pin_memory=True, num_workers=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP1dv7e0PV-o",
        "outputId": "1f3dcb68-99be-4eb0-870f-c90be1a52248"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19999 4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resnet\n",
        "best of resnet18 -> freeze 70"
      ],
      "metadata": {
        "id": "fQvDJYGMV75i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfclf = RandomForrestCLF('resnet18', 70, full_loaders)\n",
        "for n_estimators in [5, 10, 20, 50]:\n",
        "    for max_depth in [10, 50, 100, 200]:\n",
        "        rfclf.train_clf(n_estimators, max_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frj2emslz8Q5",
        "outputId": "33db6422-4bda-4a3d-9a31-b912a7098749"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 10\n",
            "acc:  0.9783956791358271\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 50\n",
            "acc:  0.9779955991198239\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 100\n",
            "acc:  0.9779955991198239\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 200\n",
            "acc:  0.9779955991198239\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 10\n",
            "acc:  0.9813962792558512\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 50\n",
            "acc:  0.9827965593118624\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 100\n",
            "acc:  0.9827965593118624\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 200\n",
            "acc:  0.9827965593118624\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 10\n",
            "acc:  0.9841968393678736\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 50\n",
            "acc:  0.9839967993598719\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 100\n",
            "acc:  0.9839967993598719\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 200\n",
            "acc:  0.9839967993598719\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 10\n",
            "acc:  0.9851970394078816\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 50\n",
            "acc:  0.9875975195039007\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 100\n",
            "acc:  0.9875975195039007\n",
            "using model resnet18 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 200\n",
            "acc:  0.9875975195039007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Densenet\n",
        "best of densenet121 -> freeze 30"
      ],
      "metadata": {
        "id": "9KP7aqvhWADs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfclf = RandomForrestCLF('densenet121', 70, full_loaders)\n",
        "for n_estimators in [5, 10, 20, 50]:\n",
        "    for max_depth in [10, 50, 100, 200]:\n",
        "        rfclf.train_clf(n_estimators, max_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XBHZQqPV0EN",
        "outputId": "62670ad6-a75f-43a6-dc5b-32d86d1cd0bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 10\n",
            "acc:  0.9751950390078016\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 50\n",
            "acc:  0.9749949989997999\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 100\n",
            "acc:  0.9749949989997999\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 200\n",
            "acc:  0.9749949989997999\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 10\n",
            "acc:  0.9817963592718544\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 50\n",
            "acc:  0.9809961992398479\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 100\n",
            "acc:  0.9809961992398479\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 200\n",
            "acc:  0.9809961992398479\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 10\n",
            "acc:  0.9837967593518704\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 50\n",
            "acc:  0.9847969593918784\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 100\n",
            "acc:  0.9847969593918784\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 200\n",
            "acc:  0.9847969593918784\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 10\n",
            "acc:  0.9853970794158832\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 50\n",
            "acc:  0.9867973594718944\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 100\n",
            "acc:  0.9867973594718944\n",
            "using model densenet121 with freeze_mode 70 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 200\n",
            "acc:  0.9867973594718944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG\n",
        "best of vgg16 -> freeze 30"
      ],
      "metadata": {
        "id": "peZF4xstWJzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfclf = RandomForrestCLF('vgg16', 30, full_loaders)\n",
        "for n_estimators in [5, 10, 20, 50]:\n",
        "    for max_depth in [10, 50, 100, 200]:\n",
        "        rfclf.train_clf(n_estimators, max_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT855ExWWYB5",
        "outputId": "cc9cc3ab-abf5-4481-854b-08294a063b7e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 10\n",
            "acc:  0.9585917183436687\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 50\n",
            "acc:  0.9587917583516703\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 100\n",
            "acc:  0.957991598319664\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 5, max_depth 200\n",
            "acc:  0.957991598319664\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 10\n",
            "acc:  0.9707941588317663\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 50\n",
            "acc:  0.9729945989197839\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 100\n",
            "acc:  0.9737947589517904\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 10, max_depth 200\n",
            "acc:  0.9737947589517904\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 10\n",
            "acc:  0.9779955991198239\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 50\n",
            "acc:  0.9807961592318464\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 100\n",
            "acc:  0.9815963192638528\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 20, max_depth 200\n",
            "acc:  0.9815963192638528\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 10\n",
            "acc:  0.9807961592318464\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 50\n",
            "acc:  0.9851970394078816\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 100\n",
            "acc:  0.9861972394478896\n",
            "using model vgg16 with freeze_mode 30 as feature extractor...\n",
            "RF params: n_estimators 50, max_depth 200\n",
            "acc:  0.9861972394478896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77XMjmYXQSOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bqf0gA-OQSQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old data loading code"
      ],
      "metadata": {
        "id": "ofAM10bjQ4YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class customDS(Dataset):\n",
        "#     def __init__(self, data, labels, transforms=None):\n",
        "#         self.data = data\n",
        "#         self.labels = labels\n",
        "#         self.transforms = transforms\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         if self.transforms is None:\n",
        "#             return self.data[idx], self.labels[idx]\n",
        "#         else:\n",
        "#             return self.transforms(self.data[idx]), self.labels[idx]\n",
        "\n",
        "# def get_dataloaders(batch_size: int = 8, randseed: int = 11, pret_model_data: str = 'ImageNet'):\n",
        "#     \"\"\"\n",
        "#     if not customDataSet is saved on disk:\n",
        "#         create ImageFolder Dataset, \n",
        "#         Read and transform Data,\n",
        "#         save the data into list,\n",
        "#         create customDataSet from the lists,\n",
        "#         torch save the customDataSets,\n",
        "#     torch load customDataSets from disk,\n",
        "#     create data loaders\n",
        "#     return data loaders \n",
        "#     \"\"\"\n",
        "#     if not (os.path.isfile('dataset/train_ds.pt') \\\n",
        "#             and os.path.isfile('dataset/test_ds.pt')):\n",
        "\n",
        "#         full_loaders_1, _ = get_data(256, randseed=randseed, \\\n",
        "#                                      pin_memory=True, pret_model_data=pret_model_data)\n",
        "\n",
        "#         data_train = []\n",
        "#         y_train = []\n",
        "#         data_test = []\n",
        "#         y_test = []\n",
        "#         for phase in ['train', 'val']:\n",
        "#             idx = 0\n",
        "#             for inputs, labels in tqdm(full_loaders_1[phase]):\n",
        "#                 idx += 1\n",
        "#                 if phase == 'train':\n",
        "#                     data_train.append(inputs)\n",
        "#                     y_train.append(labels)\n",
        "#                 else:\n",
        "#                     data_test.append(inputs)\n",
        "#                     y_test.append(labels)\n",
        "#                 # if idx == 5:\n",
        "#                 #     break \n",
        "         \n",
        "#         train_ds_temp = customDS(torch.cat(data_train), torch.cat(y_train))\n",
        "#         test_ds_temp  = customDS(torch.cat(data_test), torch.cat(y_test))\n",
        "\n",
        "#         mkdir('dataset')\n",
        "#         torch.save(train_ds_temp, 'dataset/train_ds.pt')\n",
        "#         torch.save(test_ds_temp, 'dataset/test_ds.pt')\n",
        "\n",
        "#     train_ds = torch.load('dataset/train_ds.pt')\n",
        "#     test_ds  = torch.load('dataset/test_ds.pt')\n",
        "\n",
        "#     def seed_worker(worker_id):\n",
        "#         # worker_seed = torch.initial_seed() % 2 ** 32\n",
        "#         np.random.seed(randseed)\n",
        "#         random.seed(randseed)\n",
        "\n",
        "#     g = torch.Generator()\n",
        "#     g.manual_seed(randseed)\n",
        "\n",
        "#     full_data_loaders = {\n",
        "#         'train': DataLoader(train_ds, batch_size=batch_size, \n",
        "#                             worker_init_fn=seed_worker,\n",
        "#                             generator=g,\n",
        "#                             shuffle=True,\n",
        "#                             # pin_memory=pin_memory\n",
        "#                             ),\n",
        "#         'val': DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
        "#                           # pin_memory=pin_memory\n",
        "#                           )\n",
        "#     }\n",
        "\n",
        "#     return full_data_loaders"
      ],
      "metadata": {
        "id": "qlEeDDUUQSTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}